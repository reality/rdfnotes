\documentclass{article}
\usepackage{listings}
\usepackage{graphicx}

\lstset{ %
language=Java,                  % the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it's 1, each line 
                                % will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                   % adds a frame around the code
tabsize=2,                      % sets default tabsize to 2 spaces
captionpos=t,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
title=\lstname,                 % show the filename of files included with \lstinputlisting;
}

\begin{document}

\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}

\title{Extending SPARQL With Remote OWL Reasoning}
\author{Luke Slater (lus11@aber.ac.uk)}
\date{February 2014}

\maketitle

\pagebreak

4308

\section{Introduction}

My project is concerned with interlinking two semantic web technologies,
allowing reasoning to take place in a remote system from a local endpoint and
allowing these results to be utilised by the initial query. This is undertaken
with a view to increase the interoperability of semantic web technologies, and
increase the power and usefulness of the federation they work in.

Specifically, the project will be the creation of an addition to
SPARQL\cite{sparql} (SPARQL Protocol And RDF Query Language) to allow remote
OWL\cite{owlprimer} reasoning to be performed on OWLlink endpoints and be included in the
SPARQL result-set.

There is a similar construct already included in the SPARQL language called
\emph{SERVICE}, which is unfortunately not suitable for use in this project
because it won't support the syntax of the OWL language within it (however, it
might be worth contacting the developers of the SPARQL specification asking if a
generalisation would be prudent).

So, I will develop a similar addition to the language, such as an \emph{OWL}
block, which will allow the language to execute a given OWL query at a given
OWLlink endpoint (along with the possibility of a flag depending on the type of
SPARQL query), and then retrieve these results and use them in the SPARQL
result-set.

Furthermore, I will endeavour to create a working implementation of this
addition - which will allow a demonstration of the system. This will require
some research into the various technologies currently available.

Another particular focus of my resulting work, beyond the primary functionality
will be in creating software that is easy to maintain, extend and modify - in
the spirit of open data and the semantic web.

Upon finishing the research project, the findings will be presented in the form
of a scientific paper presenting the specification of the addition to the
language, its impact and its uses.

\section{State of The Art in The Semantic Web}

The Semantic Web is a set of technologies and methodologies suitable for
generalised data expression, transmission and processing over the World Wide
Web. Before this, up until the Semantic Web's philosophical beginnings in
2001, the World Wide Web had seen success as a platform for the
sharing resources intended for reading and interaction by humans.

This meant that most of the data existed in the format of forward-facing 
documents accessible via URLs, content comprising of arbitrary natural language
along with human-intended formatting and navigation techniques.

The downside of this, is that without using advanced natural language 
processing techniques, it's very difficult to then have software process and
intercommunicate this data in a generalised and useful manner.

Therefore, the philosophy of the semantic web acknowledges this need for a
reconciliation of certain data in a format which software can deal in. The
Semantic Web\cite{semweb} describes the vision of a futuristic world of home
automation and service intercommunication serving the human lifestyle, backed by
these data formats which allow software to easily traverse the totality of
available data and process it in a meaningful manner. 

To move forward and implement the Semantic web, we have since that date seen the
production of a number of technologies moving towards this goal.

\subsection{Technologies}

\subsubsection{RDF}

One of the primary technologies used to represent this machine-workable data on
the web is the Resource Description Framework (RDF), which are a set of
specifications which describe a general methodology for conceptualising and
modelling data, intended for solutions in which the data needs to be processed
by applications.

It forms a graph of statements representing metadata about resources on the
World Wide Web in the form of simple statements in the terms of simple
properties and property values. Each RDF statement is formed of a Subject, a
Predicate and an Object:

\begin{description}
    \item[Subject] The object being described.
    \item[Predicate] The definition of the property of the subject being
    defined - usually given as a URI reference or URIRef.
    \item[Object] This is the value assigned to the given
    predicate-defined property of the subject.
\end{description}

For example, in the simplest terms a statement which defines the name of the author of
this document might take the form:

\begin{lstlisting}
http://users.aber.ac.uk/lus11/dissertation has an author whose value is Luke
Slater.
\end{lstlisting}

In which the subject is \emph{http://users.aber.ac.uk/lus11/dissertation}, the
predicate is \emph{author} and the object is \emph{Luke Slater}.

Usually, URIs are used to identify objects, subjects and predicates because they
allow them to be fully-fledged resources and in using already-defined
vocabularies one can avoid using multiple strings to refer to the same thing.

The actual RDF datastore, may look like the following:

\begin{lstlisting}
<?xml version="1.0"?>

<rdf:RDF
xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
xmlns:si="http://www.w3schools.com/rdf/">

<rdf:Description rdf:about="http://users.aber.ac.uk/lus11/dissertation">
  <si:author>Luke Slater</si:author>
</rdf:Description>

</rdf:RDF> 
\end{lstlisting}

RDF information is commonly stored, transmitted and worked with using the
RDF/XML format, which is an expression of the graph data structures in the
eXtensible Markup Language (XML) - a language designed to store and encode
documents (as opposed to HTML, which is for formatting data). It may also be
stored in a JSON (which is a notation more suited for native key-value stores),
but what these all hold in common are their ease of computer-operability; such
data expression formats are widely adopted and there exist hundreds of thousands
of applications which rely on 

While XML and simliar already exist, and allow for easy storage, transmission 
and manipulation by machines, there is an issue in that applications which use
it often define their own schemas and constructs for data description. This is
useful for the application itself and for applications which subscribe to its
data schema, but requires extra work for applications which wish to use it.

While key value data stores have many advantages over Relational Database
Management Systems (RDBMS), they have the advantage that for certain datatypes,
for example a 'date' are centrally configured and understood by all instances of
that database driver (MySQL, for example). Key-value data is much more freeform,
consisting of nothing but a key and a value. 
well as extra documentation being required for each piece of software to
describe its data format and even the types of data. 

RDF strives to solve these problems, creating a universal and general
specification for resource description, by providing a known data format with
widely known vocabularies included in predicates within the data to describe 
the datatypes within it - the existence and extensibility of said vocabularies 
meaning it can universally define and describe much more complicated and 
specific datatypes than can RDBMS systems.

* triples

\subsubsection{OWL Ontologies}

An ontology is a formal representation of a set of concepts and their
interrelationships, types and hierarchical standings. RDF is rather naturally
suited to storing this type of data, being that it represents a graph, however
technologies such as OWL (The Web Ontology Language) have been developed
post-RDF to provide additional suitability and a greater featureset for this
paradigm of knowledge representation.

Despite the misleading acronym, OWL itself actually includes a large amount of
different individual languges depending on the version of the specification used
and the set of features required from the knowledge representation.

Both ontologies and OWL ontologies specifically include a set of classes, which 
describe objects within the appropriate domain (much like a declarative RDF 
description of an object with a set of predicates and values). It also contains
a set of axioms, or constraints, which describe the interrelationships between
the classes.

The set of available axioms, constraints and relationships available in OWL
ontologies allow it to effectively model description logic\cite{desclogic}. 

** more about description logic.

This is supported by semantic reasoners, which are pieces of software designed
to evaluate the set of axioms between classes for logical consistency. OWL
succeeds many previous attempts at ontological representation due to the
existence of complete and terminating reasoners which can fully evaluate every
consequence of the logic which exists in an ontology, allowing larger and more
complicated ontologies to exist while continuing to contain validated 
consistent meaning.

* Describe OWL
* Description Logic
* Semantic reasoners

\subsubsection{SPARQL}

Previously, languages used for the representation of various types of ontologies
were described, these are designed to be interacted with by machines - however,
a number of technologies also exist for human-interaction with the datastores
including querying and data manipulation.

SPARQL (SPARQL Protocol And RDF Query Language) is a query language similar to
SQL which allows the querying and manipulation of RDF (and similar) datastores.
This is a human-interactable language, which means that while it has a
non-freeform syntax it is designed to be used by humans (as well as computers)
to interact with data on the semantic web.

SPARQL queries usually consist of the following:

\begin{description}
    \item[PREFIX] A list of prefix declarations, which can be used to shorten
    and simplify URIs in the remainder of the query.
    \item[FROM] Describes the dataset to query, usually a URI pointing at an RDF
    dataset.
    \item[SELECT] Describes the data to include in the resultset.
    \item[WHERE] Describes the data to query for in the dataset.
    \item[MODIFIERS] These allow you to apply modifiers to the resultset, such
    as a limitation on the total results or a pattern for ordering.
\end{description}

Less commonly there are also CONSTRUCT, which allows you to pull full triples of
data from a store.

SPARQL also has the provision to query data from non-RDF databases such as Redis
and OWL ontologies by transforming and interacting with the data as if it were
in an RDF syntax. This is useful for data integration, as it means data can be
used over multiple formats. 

Another important quality of SPARQL is that it's federated, data may be loaded
remotely over the web by providing an IRI to the datastore. Also, the language
includes a 'SERVICE' keyword, which allows a query to be sent to a remote SPARQL
endpoint and be executed and results be returned and included in the resultset.

A SPARQL endpoint is a simple interface which accepts a HTTP query including a
query parameter, then runs the query and will return the results in one of 
several machine-readable formats (usually JSON or XML). This means a wide array
of applications can easily make use of the data SPARQL deals in - one example
being Virtuoso. It also means that simple online interfaces can be developed for 
working with SPARQL. These qualities mean that SPARQL makes RDF very universally
available for use.

\subsubsection{Manchester OWL Syntax}

Manchester OWL Syntax is a class of OWL language which is designed to be
human-friendly, it can be used to represent full ontologies - though a certain
few applications such as Protoge utilise the simple syntax to allow users to
return classes relevant to a given expression.

By default, the syntax deals in IRIs, however implementations may be modified to
deal in labels as a short-form for an even more compact syntax.

\subsection{Frameworks and Implementations} 

\subsubsection{Jena}

Jena is a large semantic web framework for Java, which provides an API for
reading several technologies including RDF and OWL, and its main uses are
the extraction of data from various database systems and re-representing them in
an RDF format. 

It includes ARQ, which is an implementation of the SPARQL query language
However, while Jena is a large and well supported application, it suffers 
from a lack of extensibility, evidenced by a very large and widely undocumented 
codebase. This means that it's rather difficult to use its implementation of
SPARQL for additions or modifications to the syntax.

\subsubsection{SPARQL Endpoints}

\subsubsection{OWLAPI}

OWLAPI is a Java library framework for manipulating OWL ontologies for a number of
purposes, and is the reference implementation for the OWL standard from W3C. 
It also allows users to load, manipulate and reason with OWL ontologies and has 
support for several manners of OWL languages and syntaxes, converting these to 
normative OWL class expressions. It also supports reasoning ontologies with all
of the major semantic reasoners (FaCT++, HermiT, Pellet etc).

Many other OWL-involved technologies use this framework, including OWLLink and Jena's 
OWL components. As a client library it is well supported and well documented
(with a full set of JavaDoc), with much sample code available.

There are alternatives for client libraries involved in manipulating OWL
ontologies, many individual reasoners include their own libraries - however the
advantage of OWLAPI is such that it supports all of the reasoners, improving
the level of universal application and reduction in work for extension in
functionality; such libraries also do not support the full set of OWL languages,
including Manchester OWL Syntax which is a human-interactable language for
querying and representing OWL ontologies.

\subsubsection{OWLLink and OWLLink API}

OWLLink is a specification which defines a machine-usable API for OWL reasoners,
the reference implementation for which is OWLLink API. The OWLLink API provides
client applications using OWLAPI to access remote reasoners through providing a
server adapter to an OWLAPI-abstracted reasoner. 

The disadvantage of this software is that it requires the client application
to use OWLAPI, or otherwise requires a complicated XML strucuture for the
request of data - a normalised expression of an OWL class expression. This
represents somewhat of a lock-in for the client software. 

Additionally, it does not accept Manchester OWL Syntax for queries - this could 
be remedied by parsing Manchester OWL Syntax into a class description understood 
by an OWLLink server on the clientside (a provision made available by OWLAPI).

Alternatively, a wrapper around the OWLLink server could be made, using OWLAPI
to convert the Manchester OWL Syntax to the class expression, then passing it to
the OWLLink server. However, this is rather inefficient as it would require an
extra external HTTP endpoint. The approach of creating a new piece of software 
based on OWLAPI with a simple HTTP server would be, in effect, cutting out the 
middleman.

\subsection{Uses and Impact}

As these technologies and implentations exist, they have mostly been utilised
for scientific purposes.

One thing that seems lacking in implementations of semantic web technologies as
they exist is that the major implementations are all rather large and opaque
Java libraries - this being a language which generally requires a lot of
boilerplate - additionally, transmission formats are commonly XML, which also
requires extra overhead in processing when compared to abstract key-value
storage formats such as JSON. 

Conversely, underlying technologies and applications on the web have meanwhile 
been moving towards use of lightweight frameworks and languages. Modern applications 
have been opting for abstract key-value stores such as Redis, which suffers from
a lack of additional features available from ontologies. 

Transmission of data on the web tends to favour simple REST APIs which feature 
simple HTTP requests with parameters, instead of the complicated XML structures 
necessary to query data from OWLLink servers for example. On the other hand, the
previously discussed SPARQL data endpoints provide a simple HTTP interface for
SPARQL queries, and I believe the effectiveness of the availability of these
simpler and more accessible technologies can be seen in the difference in uptake
between SPARQL and OWLLink.

Therefore, I believe it may be that the lack of takeup of knowledge-representation systems
for front-end applications may be the down to the lack of lightweight libraries, 
services and frameworks available for working with them.

\section{Problem Statement}

\subsection{Data Integration}

The issue with the Semantic Web as it currently exists is that while there is a
lot of data stored in machine-processable formats, there is a lot of information 
existing in different data models (RDF and OWL ontologies, in respects to this project). 
This means it becomes somewhat difficult to integrate data between said models 
if they are representing similar data in different ways.

There is already some provision for this, as explained in the previous section - 
SPARQL does actually allow data to be queried from an OWL ontology, loading a full 
list of objects in an ontology designated by an IRI and then being able to query 
it in an RDF expression. However, while this is useful in some cases, it misses 
two major advantages of OWL ontologies: the performance of semantic reasoning
upon the dataset, and the ability to query the data in a native manner (using 
Manchester OWL Syntax).

Therefore the problem in this case is developing a manner by which to integrate
data stored in RDF and OWL formats without compromising the endemic qualities of 
each model through doing so.

* Why it's useful

\subsection{Human-interactable OWL Querying Over The Web}

As described in the State of The Semantic Web, there are two categories of
languages used to represent and interact with the data: human-interactable and
machine-interactable. These generally exist as the data storage formats (such as
RDF or OWL) and the query languages (such as SPARQL or Manchester OWL Syntax)
which can be used by humans to manipulate and integrate the datastores.

An issue with the Semantic Web is also that of exposing these datasets to use by
computers and machines over the Web i.e. data avilability and integration over
the web.

For both RDF and OWL datastores it is currently easy to interact with the data
over the web in a machine-processable manner - primarily through codebases
utilising XML parsers or through bespoke libraries and server software such as 
OWLAPI and OWLLink.

For human-querying, the area is highly-developed for RDF and similar datastores, 
with SPARQL allowing users to write queries to manipulate data both locally and
through web-based endpoints. The language also supports federation through the
SERVICE keyword, which allows the user to send further data queries to remote
SPARQL endpoints and thereby further datasets. This is incredible useful for
making useful data easily available and integratable to those working with it.

However, for OWL ontologies the situation is somewhat different,
human-interaction with ontologies is generally done through desktop applications
such as Protégé, which allows users to load, edit, reason and query ontologies 
using the Manchester OWL Syntax.

However, there is currently no way for users to send simple Manchester OWL
Syntax queries to remote reasoners and retrieve relevant classes for use. 
Therefore, another problem is providing a web-based solution for human-input OWL
queries over multiple ontologies through Web. 

* semantic web lacking simple software

\section{Design and Implementation}

My solution to the given problems are to develop an extension to the SPARQL
query language which allows a user to send a Manchester OWL Syntax query to a
remote OWL reasoner endpoint, which will return relevant objects which are
included in the SPARQL resultset. This will require both development of the
language extension and the queriable OWL endpoint.

\subsection{SPARQL Extension}

\subsubsection{Design}

As described in the State of The Art, there is already a SERVICE keyword
included in the SPARQL language which provides a good model for sending a text
query to a remote endpoint. Currently, it only supports remote SPARQL endpoints,
and it was decided not to extend this construct to include support for an OWL
endpoint as this would mean any resulting implementation would break the terms
of the SPARQL specification. However, it may be suggested to the W3C in future
to combine these constructs as a matter of efficiency. 

Therefore, my software will add an 'OWL' block to the SPARQL language, which
uses a similar syntactic pattern to the SERVICE keyword.

\begin{lstlisting}
OWL SILENT <http://realispicio.us:9090> {
    Pizza and hasTopping some FishTopping
}
\end{lstlisting}

There are three properties to the above query:

\begin{description}
    \item[OWL] The OWL keyword will designate the beginning of an OWL block
    denoting the beginning of the description for a query to be sent to a remote
    OWL endpoint.
    \item[SILENT] This is an optional keyword, which if included will mean that
    no results are integrated if a query fails but the query will continue,
    while if it isn't set and a query fails the SPARQL query will be halted and
    return an error message.
    \item[IRI] The IRI, contained between angled brackets following the OWL
    keyword, will define the URI of the remote OWL endpoint.
    \item[Query] Following, between the curly brackets, will be a Manchester OWL
    Syntax query referring to relevant objects to be returned to the query.
\end{description}

Upon parsing the OWL block the syntax will send a query to an OWL reasoner
endpoint (described next in this section), and integrate the returned results
into the query. 

\subsubsection{Implementation}

In researching the implementation of the OWL block addition to SPARQL, the
initial thoughts were to perform this by actually extending the language itself.
To do this, the only seemingly viable option was to extend Apache Jena, which is
a major and widely used semantic web framework written in Java which includes the 
ARQL SPARQL compiler. 

The original plan was to build the extra syntax into ARQL in Jena, and
distribute a modified version of the software while requesting the changes be
merged with the main codebase.

However, after further investigation this approach was deemed unsuitable because
of the perceived inextensibility of the ARQL codebase, which makes use of a
large amount of magic numbers and goes largely undocumented. [am I allowed to
say this?]

Therefore, a simpler approach was taken - to extend a library which sends a
SPARQL query to a SPARQL endpoint by preprocessing it to find and run OWL
blocks, then replacing the block with the results inline. The workflow of the
script will be the following:

\begin{enumerate}
    \item Input the SPARQL query into the script.
    \item Scan the script for OWL blocks (syntax described above).
    \item Run any OWL queries.
    \item Replace the OWL block with the resultset from the OWL query.
    \item Run the SPARQL query.
    \item Return the full resultset.
\end{enumerate}

In terms of inputting the SPARQL query into the processing script, it will
support being run both from the command line and over the web, by checking how
it's being run. From there, it will gain the input run from the command line
either designated in plaintext in the arguments, or from a file named in the
arguments; if run over the web, it will accept a POST parameter including the
query sent from a simple web interface also implemented by me (described later).

Scanning the script for occurences of OWL blocks will be done with a simple
Regex to build an array of all matches and involved parameters.

It will use these parameters to run each OWL block against the given OWL 
endpoint.

These results will then replace the OWL block in-place in the SPARQL query,
using the VALUES inline data syntax.

Once these have all been performed, the SPARQL query will be run against a
SPARQL endpoint and the result returned to the user using the appropriate method.

As an example, a simple SPARQL query including an OWL block like so (which
simply asks for classes related to the adaxial cell in a zebrafish ontology):

\begin{lstlisting}
SELECT * WHERE {}
OWL <http://realispicio.us:9090> {
    'adaxial cell'
}
\end{lstlisting}

Would first be processed to a SPARQL query string containing the OWL block
replaced by a VALUES block containing the results of the query:

\begin{lstlisting}
SELECT * WHERE {} 
VALUES (?iri ?id) { 
    ( http://purl.obolibrary.org/obo/#ZFA_0009000  ZFA_0009000 ) 
    ( http://www.w3.org/2002/07/owl#Nothing  Nothing )
}
\end{lstlisting}

\subsection{OWL Endpoint}

\subsubsection{Design}

The OWL endpoint will be a simple HTTP REST-like API which wraps code around an
OWL reasoner. The software will load a set of ontologies which any queries given
to the server will be run against, run the reasoner against them, and then set
up a simple server to accept Manchester OWL Syntax requests and return the
results in a machine-readable format.

\subsubsection{Implementation}

The initial stages of the program startup will be the following:

\begin{enumerate}
    \item Load given ontologies into an ontology manager. 
    \item Perform reasoning on a set of ontologies and store the results.
    \item Generate a list of short-form labels for the classes in the
    ontologies.
    \item Start a HTTP API listener with code to handle any queries.
\end{enumerate}

The workflow of a request made to the server will be as follows. 

\begin{enumerate}
    \item Retrieve Manchester OWL Syntax query.
    \item Convert the Manchester OWL Syntax query to a class expression (able to
    be queried against an ontology).
    \item Retrieve relevant objects from the ontologies.
    \item Serialise these objects in a machine-readable format and return them 
    to the collector.
\end{enumerate}

OWLAPI will be used to handle ontologies, including the reasoning, Manchester
OWL Syntax conversion and class querying. 

For the server software I decided to use Jetty, since my research showed that
this was about the simplest HTTP server library available in Java which
implements the required functionality. It is well documented, and uses many
in-built Java API classes which works towards keeping the codebase light and
easily extensible by others. 

* not extensible
* should be on the side of the server

* Insert class diagram
* Workflow

\section{Testing}

\section{Discussion}

\section{Project Deliverables}

\begin{itemize}
  \item \emph{Specification} - This will involve a description of the language additions 
  and their syntax in a 'specification'-type format. 
  \item \emph{Implementation} - An implementation of the above addition in the
  form of an addition to a piece of software which iplements a SPARQL server to
  allow for the running of the queries detailed above.
  \item \emph{Working Demonstration} - In the form of a working SPARQL endpoint
  and an OWLlink endpoint, with the ability for the former to run queries on the
  latter through the previously developed language extension.
  \item \emph{Tests} - A couple of tests (either manual or automatic) which
  prove the extensions as working.
  \item \emph{Progress Report} - This involves a mid-period report which covers
  an overview of the technologies learned, and details on the specification and 
  implementation of the extension to SPARQL.
  \item \emph{Paper} - Scientific paper describing the results of the research
  project.
  \item \emph{Final Report} - Report covering the entire project, including
  a full project description, background, impact and significance, 
  implementation process, documentation and evaluation of final progress.
\end{itemize}

\section{Conclusion}

* Future
* Directions for the project itself
* What can build on it

\begin{thebibliography}{9}

\bibitem{semweb}
  Tim Berners-Lee et al,
  \emph{The Semantic Web}.
  Feature Article, Scientific American
  May 2001.

\bibitem{rdfprimer}
 Tim Berners-Lee et al,
 \emph{RDF Primer}.
 W3C,
 2004.

\bibitem{sparql}
  Eric Prud'hommeaux, Andy Seaborne,
  \emph{SPARQL Query Language for RDF}.
  W3C,
  January 2008.

\bibitem{desclogic}
 Franz Baader, Deborah L. McGuinness, Daniele Nardi, Peter F. Patel-Schneider,
 \emph{The Description Logic Handbook - Theory, Implementation and Actions}.
 Cambridge University Press,
 March 2003.

\bibitem{owlprimer}
  Pascal Hitzler et al,
  \emph{OWL 2 Web Ontology Language Primer (Second Edition)}.
  W3C,
  December 2012.

Example OWLLink query:
http://owllink.org/owllink-httpxml-20091116/additional-examples/owllink-example-ClassQueries-request-20091116.xml

\end{thebibliography}

\end{document}
