\documentclass{article}
\usepackage{listings}
\usepackage{graphicx}

\lstset{ %
language=Java,                  % the language of the code
basicstyle=\footnotesize,       % the size of the fonts that are used for the code
numbers=left,                   % where to put the line-numbers
numberstyle=\footnotesize,      % the size of the fonts that are used for the line-numbers
stepnumber=1,                   % the step between two line-numbers. If it's 1, each line 
                                % will be numbered
numbersep=5pt,                  % how far the line-numbers are from the code
showspaces=false,               % show spaces adding particular underscores
showstringspaces=false,         % underline spaces within strings
showtabs=false,                 % show tabs within strings adding particular underscores
frame=single,                   % adds a frame around the code
tabsize=2,                      % sets default tabsize to 2 spaces
captionpos=t,                   % sets the caption-position to bottom
breaklines=true,                % sets automatic line breaking
breakatwhitespace=false,        % sets if automatic breaks should only happen at whitespace
title=\lstname,                 % show the filename of files included with \lstinputlisting;
}

\begin{document}

\setlength{\parskip}{\medskipamount}
\setlength{\parindent}{0pt}

\title{Extending SPARQL With Remote OWL Reasoning}
\author{Luke Slater (lus11@aber.ac.uk)}
\date{February 2014}

\maketitle

\pagebreak

\section{Introduction}

My project is concerned with interlinking two semantic web technologies,
allowing reasoning to take place in a remote system from a local endpoint and
allowing these results to be utilised by the initial query. This is undertaken
with a view to increase the interoperability of semantic web technologies, and
increase the power and usefulness of the federation they work in.

Specifically, the project will be the creation of an addition to
SPARQL\cite{sparql} (SPARQL Protocol And RDF Query Language) to allow remote
OWL\cite{owlprimer} reasoning to be performed on OWLlink endpoints and be included in the
SPARQL result-set.

There is a similar construct already included in the SPARQL language called
\emph{SERVICE}, which is unfortunately not suitable for use in this project
because it won't support the syntax of the OWL language within it (however, it
might be worth contacting the developers of the SPARQL specification asking if a
generalisation would be prudent).

So, I will develop a similar addition to the language, such as an \emph{OWL}
block, which will allow the language to execute a given OWL query at a given
OWLlink endpoint (along with the possibility of a flag depending on the type of
SPARQL query), and then retrieve these results and use them in the SPARQL
result-set.

Furthermore, I will endeavour to create a working implementation of this
addition - which will allow a demonstration of the system. This will require
some research into the various technologies currently available.

Another particular focus of my resulting work, beyond the primary functionality
will be in creating software that is easy to maintain, extend and modify - in
the spirit of open data and the semantic web.

Upon finishing the research project, the findings will be presented in the form
of a scientific paper presenting the specification of the addition to the
language, its impact and its uses.

\section{State of The Art in The Semantic Web}

The Semantic Web is a set of technologies and methodologies suitable for
generalised data expression, transmission and processing over the World Wide
Web. Before this, up until the Semantic Web's philosophical beginnings in
2001, the World Wide Web had seen success as a platform for the
sharing resources intended for reading and interaction by humans.

This meant that most of the data existed in the format of forward-facing 
documents accessible via URLs, content comprising of arbitrary natural language
along with human-intended formatting and navigation techniques.

The downside of this, is that without using advanced natural language 
processing techniques, it's very difficult to then have software process and
intercommunicate this data in a generalised and useful manner.

Therefore, the philosophy of the semantic web acknowledges this need for a
reconciliation of certain data in a format which software can deal in. The
Semantic Web\cite{semweb} describes the vision of a futuristic world of home
automation and service intercommunication serving the human lifestyle, backed by
these data formats which allow software to easily traverse the totality of
available data and process it in a meaningful manner. 

To move forward and implement the Semantic web, we have since that date seen the
production of a number of technologies moving towards this goal. Today, the
Semantic Web exists as a movement promoting standardised structured data
formats, primarily led by W3C.

\subsection{Technologies}

\subsubsection{RDF}

One of the primary technologies used to represent this machine-workable data on
the web is the Resource Description Framework (RDF), which are a set of
specifications which describe a general methodology for conceptualising and
modelling data, intended for solutions in which the data needs to be processed
by applications.

It forms a graph of statements representing metadata about resources on the
World Wide Web in the form of simple statements in the terms of simple
properties and property values. Each RDF statement is formed of a Subject, a
Predicate and an Object:

\begin{description}
    \item[Subject] The object being described.
    \item[Predicate] The definition of the property of the subject being
    defined - usually given as a URI reference or URIRef.
    \item[Object] This is the value assigned to the given
    predicate-defined property of the subject.
\end{description}

For example, in the simplest terms a statement which defines the name of the author of
this document might take the form:

\begin{lstlisting}
http://users.aber.ac.uk/lus11/dissertation has an author whose value is Luke
Slater.
\end{lstlisting}

In which the subject is \emph{http://users.aber.ac.uk/lus11/dissertation}, the
predicate is \emph{author} and the object is \emph{Luke Slater}.

Usually, URIs are used to identify objects, subjects and predicates because they
allow them to be fully-fledged resources and in using already-defined
vocabularies one can avoid using multiple strings to refer to the same thing.

The actual RDF datastore, may look like the following:

\begin{lstlisting}
<?xml version="1.0"?>

<rdf:RDF
xmlns:rdf="http://www.w3.org/1999/02/22-rdf-syntax-ns#"
xmlns:si="http://www.w3schools.com/rdf/">

<rdf:Description rdf:about="http://users.aber.ac.uk/lus11/dissertation">
  <si:author>Luke Slater</si:author>
</rdf:Description>

</rdf:RDF> 
\end{lstlisting}

RDF information is commonly stored, transmitted and worked with using the
RDF/XML format, which is an expression of the graph data structures in the
eXtensible Markup Language (XML) - a language designed to store and encode
documents (as opposed to HTML, which is for formatting data). It may also be
stored in a JSON (which is a notation more suited for native key-value stores),
but what these all hold in common are their ease of computer-operability; such
data expression formats are widely adopted and there exist hundreds of thousands
of applications which rely on 

While XML and simliar already exist, and allow for easy storage, transmission 
and manipulation by machines, there is an issue in that applications which use
it often define their own schemas and constructs for data description. This is
useful for the application itself and for applications which subscribe to its
data schema, but requires extra work for applications which wish to use it.

While key value data stores have many advantages over Relational Database
Management Systems (RDBMS), they have the advantage that for certain datatypes,
for example a 'date' are centrally configured and understood by all instances of
that database driver (MySQL, for example). Key-value data is much more freeform,
consisting of nothing but a key and a value. 
well as extra documentation being required for each piece of software to
describe its data format and even the types of data. 

RDF strives to solve these problems, creating a universal and general
specification for resource description, by providing a known data format with
widely known vocabularies included in predicates within the data to describe 
the datatypes within it - the existence and extensibility of said vocabularies 
meaning it can universally define and describe much more complicated and 
specific datatypes than can RDBMS systems.

Additional to RDF, the RDF Schema technology was later developed. This is
designed to buld on the limited vocabulary of RDF to provide additional features
for knowledge representation by providing additional syntax for describing
classes. An example of this is the ability to describe a class's types or define
an overall hierarchy.

\begin{lstlisting}
PREFIX animal: <http://realispicio.us>
animal:turtle    rdfs:subClassOf    animal:animal
\end{lstlisting}

The above example shows the description of a cat as being a subclass of animal.
The addition of this kind of data representation constitutes description
logic\cite{desclogic} - and through doing so allows the data to actually
represent meaning through the class interrelations and restrictions (or axioms)
in an \emph{ontology}.

\subsubsection{OWL Ontologies}

An ontology is a formal representation of a set of concepts and their
interrelationships, types and hierarchical standings. RDF (and RDFS moreso) is
rather naturally suited to storing this type of data, being that it represents 
a graph, however technologies such as OWL (The Web Ontology Language) have been 
developed post-RDF to provide additional suitability and a greater featureset 
for this paradigm of knowledge representation.

Despite the misleading acronym, OWL itself actually includes a large amount of
different individual languges depending on the version of the specification used
and the set of features required from the knowledge representation, as well as
whether humans or machines are the primary audience for use. 

\begin{lstlisting}
Prefix: a: <http://realispicio.us/owl/animals/>
Ontology: <http://realispicio.us/example.owl>

ObjectProperty: eats
  Domain: Animal
  Range: Food

Class: Animal
Class: Squirrel
  SubClassOf: Animal
  EquivalentTo: eats only Nuts
Class: Turtle
  SubClassOf: Animal
  EquivalentTo: eats some People

Class Food
Class: Nuts
  SubClassOf: Food
Class: People
  SubClassOf: Food
\end{lstlisting}

Exampled above, Manchester OWL Syntax is commonly used for human-readable
representation of OWL ontologies. The example defines a simple OWL ontology with
a small set of classes and some relationships between them. Similar to RDF it
may also contain a set of prefixes usable for shortening URIs. Each ontology
must also be identified by a unique URI, stated in the Ontology declaration.

Ontologies include a set of classes, which describe objects within the appropriate 
domain (much like a declarative RDF description of an object with a set of predicates 
and values). It also contains a set of axioms, or constraints, which describe the 
interrelationships between the classes and in essence assign meaning to the
knowledge represented in the ontology.

The set of available axioms, constraints and relationships available in OWL
ontologies allow it to effectively model description logic\cite{desclogic}. 

The mainstay object relation found in knowledge representation and description
logic is that of IS-A; a relationship whereby one class is a subclass or 'type'
of another - the definition of the superclass then implying that of the
subclass. This is achieved in OWL ontologies with the 'SubClassOf' axiom,
evidenced above by describing that the Squirrel and the Turtle are both
subclasses of Animal, while Nuts and People are subclasses of Food.

Another common axiom described in ontologies are existential quantifications, or
rather, expressing constraints on a property assigned to a class. Above, we
define the object property 'eats' to all animals, noting that it can contain any
type of food. Furthermore, we then set additional constraints in the definitions
of each animal - squirrels can only eat nuts, while turtles must eat some
people.

Despite being a simple ontology, the logical implications of even a small set of
axioms are great (though not necessarily explicitly stated, especially in the
case of existential quantification). For example, an individual turtle as described 
above would be considered to pass the rules of the ontology only if they eat some 
people, therefore a turtle is free to eat nuts. On the other hand, a squirrel is limited
to eating only nuts - so a squirrel observed eating people is likely not a
squirrel at all.

The testing of the implications of the description logic used to represent
knowledge in OWL ontologies is supported by semantic reasoners, which are 
pieces of software designed to evaluate the set of axioms between classes for 
logical consistency. OWL succeeds many previous attempts at ontological 
representation due to the existence of complete and terminating reasoners 
which can fully evaluate every consequence of the logic which exists in an 
ontology, allowing larger and more complicated ontologies to exist while 
continuing to contain validated consistent meaning.

* Describe OWL
* Description Logic
* Semantic reasoners

\subsubsection{SPARQL}

Previously, languages used for the representation of various types of ontologies
were described, these are designed to be interacted with by machines - however,
a number of technologies also exist for human-interaction with the datastores
including querying and data manipulation.

SPARQL (SPARQL Protocol And RDF Query Language) is a query language similar to
SQL which allows the querying and manipulation of RDF (and similar) datastores.
This is a human-interactable language, which means that while it has a
non-freeform syntax it is designed to be used by humans (as well as computers)
to interact with data on the semantic web.

SPARQL queries usually consist of the following:

\begin{description}
    \item[PREFIX] A list of prefix declarations, which can be used to shorten
    and simplify URIs in the remainder of the query.
    \item[FROM] Describes the dataset to query, usually a URI pointing at an RDF
    dataset.
    \item[SELECT] Describes the data to include in the resultset.
    \item[WHERE] Describes the data to query for in the dataset.
    \item[MODIFIERS] These allow you to apply modifiers to the resultset, such
    as a limitation on the total results or a pattern for ordering.
\end{description}

Less commonly there are also CONSTRUCT, which allows you to pull full triples of
data from a store.

SPARQL also has the provision to query data from non-RDF databases such as Redis
and OWL ontologies by transforming and interacting with the data as if it were
in an RDF syntax. This is useful for data integration, as it means data can be
used over multiple formats. However, certain OWL representations cannot be
converted by most RDF readers - including functional syntax, Manchester syntax
and OWL/XML, and in converting an OWL ontology to RDF format, richer semantics
offered by OWL are lost - including certain axioms, and on-the-fly reasoning.

Another important quality of SPARQL is that it's federated, data may be loaded
remotely over the web by providing an IRI to the datastore. Also, the language
includes a 'SERVICE' keyword, which allows a query to be sent to a remote SPARQL
endpoint and be executed and results be returned and included in the resultset.

A SPARQL endpoint is a simple interface which accepts a HTTP query including a
query parameter, then runs the query and will return the results in one of 
several machine-readable formats (usually JSON or XML). This means a wide array
of applications can easily make use of the data SPARQL deals in - one example
being Virtuoso. It also means that simple online interfaces can be developed for 
working with SPARQL. These qualities mean that SPARQL makes RDF very universally
available for use.

\subsubsection{Manchester OWL Syntax}

Manchester OWL Syntax is a class of OWL language which is designed to be
human-friendly, it can be used to represent full ontologies - though a certain
few applications such as Protoge utilise the simple syntax to allow users to
return classes relevant to a given expression.

\begin{lstlisting}
Pizza and hasTopping some FishTopping
\end{lstlisting}

An example of how Manchester OWL Syntax for querying classes is given above, it
succinctly describes the class involved (Pizza) and then the constraint that
class must match (hasTopping some FishTopping), with Pizza classes having a
Topping subclass which may be of the instance FishTopping.

\subsection{Frameworks and Implementations} 

\subsubsection{Jena}

Jena is a large semantic web framework for Java, which provides an API for
reading several technologies including RDF and OWL, and its main uses are
the extraction of data from various database systems and re-representing them in
an RDF format. 

It includes ARQ, which is an implementation of the SPARQL query language
However, while Jena is a large and well supported application, it suffers 
from a lack of extensibility, evidenced by a very large and widely undocumented 
codebase. This means that it's rather difficult to use its implementation of
SPARQL for additions or modifications to the syntax.

\subsubsection{SPARQL Endpoints}

\subsubsection{OWLAPI}

OWLAPI is a Java library framework for manipulating OWL ontologies for a number of
purposes, and is the reference implementation for the OWL standard from W3C. 
It also allows users to load, manipulate and reason with OWL ontologies and has 
support for several manners of OWL languages and syntaxes, converting these to 
normative OWL class expressions. It also supports reasoning ontologies with all
of the major semantic reasoners (FaCT++, HermiT, Pellet etc).

Many other OWL-involved technologies use this framework, including OWLLink and Jena's 
OWL components. As a client library it is well supported and well documented
(with a full set of JavaDoc), with much sample code available.

There are alternatives for client libraries involved in manipulating OWL
ontologies, many individual reasoners include their own libraries - however the
advantage of OWLAPI is such that it supports all of the reasoners, improving
the level of universal application and reduction in work for extension in
functionality; such libraries also do not support the full set of OWL languages,
including Manchester OWL Syntax which is a human-interactable language for
querying and representing OWL ontologies.

\subsubsection{Protégé}

Protégé is an open source project which provides a desktop-based graphical user
interface for editing, building and reasoning with ontologies, which supports
the development of plugins. It is considered the premier tool for working with
OWL ontologies within the scientific community.

One of its interesting features is that of a novel use of Manchester OWL Syntax
to query classes and return those relevant (described above).

\subsubsection{OWLLink and OWLLink API}

OWLLink is a specification which defines a machine-usable API for OWL reasoners,
the reference implementation for which is OWLLink API. The OWLLink API provides
client applications using OWLAPI to access remote reasoners through providing a
server adapter to an OWLAPI-abstracted reasoner. 

The disadvantage of this software is that it requires the client application
to use OWLAPI, or otherwise requires a complicated XML strucuture for the
request of data - a normalised expression of an OWL class expression. This
represents somewhat of a lock-in for the client software. 

Additionally, it does not accept Manchester OWL Syntax for queries - this could 
be remedied by parsing Manchester OWL Syntax into a class description understood 
by an OWLLink server on the clientside (a provision made available by OWLAPI).

Alternatively, a wrapper around the OWLLink server could be made, using OWLAPI
to convert the Manchester OWL Syntax to the class expression, then passing it to
the OWLLink server. However, this is rather inefficient as it would require an
extra external HTTP endpoint. The approach of creating a new piece of software 
based on OWLAPI with a simple HTTP server would be, in effect, cutting out the 
middleman.

\subsection{Uses and Impact}

Despite the development of a great number of standards, technologies and pieces
of software since the inception of the Semantic Web, the original
dream and plan for evolution of the wide Semantic Web remains largely unrealised
for general application.\cite{semweb}

However, outside the strict prediction for the technologies developed to provide
structured data for the world wide web at large, they have been used to great
effect in specific domains - primarily in biology.

An example of a common use for OWL ontologies is that of describing animal or
plant models using the descriptive logic axioms provided by OWL. A collection 
of biological ontologies can be found at OboFoundry\cite{obofoundry}, examples
including Zebrafish models and a full plant ontology model. In working with
these ontologies the previously described Protege application is primarily used. 

These are being used for example to examine biological and drug data for medical
purposes\cite{humontology}. One particular realm of interest is drug discovery,
mining existing information on the semantic web to predict the results of drugs,
and eventually predicting and finding new targets and indications for existing
drugs\cite{semwebdiscovery}.

Despite the lack of adoption of semantic web technologies for general use, there
are a few initiatives which work to make large amounts of structured data
available. A good example of this is DBPedia, which aims to extract
information from the online encyclopaedia website Wikipedia and provide it in a
structured and machine-processable format (RDF(S)), available for free on the 
web. At the time of writing, it had collected information on more than four 
million objects and had collected more than 470 million 'facts' overall.

An important thing to note about this dataset is the grand effort which is made
to make the information freely and easily available in a number of manners and
using a number of different technologies. There is a SPARQL endpoint available
for querying, along with full database downloads and a simple endpoint for
retriving information about particular subjects in a number of formats (JSON,
XML, N-Triples etc).

The underlying technology's ability to provide simple data access over the Web
means that DBPedia can be and is already used for a number of practical
applications.\cite{dbpedia-uses}

\section{Problem Statement}

\subsection{Statement}

\emph{The issue we strive to solve is that of creating a simple solution for querying
OWL ontologies over the web.}

An issue with the usability of OWL ontologies as a technology for the
representation of knowledge on the semantic web, is that there is currently no
way to easily query data from it in a human interactable way over the Web.
Furthermore, this issue means it is difficult to integrate data from OWL
ontologies with that from RDF datasets. 

\subsection{Justification}

\subsubsection{OWL Queries}

As described in the State of The Semantic Web, there are two categories of
languages used to represent and interact with the data: human-interactable and
machine-interactable. These generally exist as the data storage formats (such as
RDF/XML or OWL/Functional) and the query languages (such as SPARQL) which can be 
used by humans to manipulate and integrate the datastores.

For both RDF and OWL datastores it is currently easy to interact with the data
over the web in a machine-processable manner - primarily through codebases
utilising XML parsers or through bespoke libraries and server software such as 
OWLAPI and OWLLink.

For human-querying, the area is highly-developed for RDF and similar datastores, 
with SPARQL allowing users to write queries to manipulate data both locally and
through web-based endpoints. The language also supports federation through the
SERVICE keyword, which allows the user to send further data queries to remote
SPARQL endpoints and thereby further datasets. This is incredible useful for
making useful data easily available and integratable to those working with it.

However, for OWL ontologies the situation is somewhat different, the 'querying'
of an ontology at all being somewhat of a novel concept - using Manchester OWL
Syntax for such queries, a purpose for which it was not intentionally designed.

Therefore the implementation of this functionality is not ubiquitous - only a
few pieces of software carrying it. One is Protege, which is a highly popular
desktop tool for interacting, building, integrating and editing OWL ontologies;
including a query tool allowing users to return relevant classes from a given
Manchester OWL Syntax string. 

However, the limitation of Protege is that it is strictly a desktop application
- missing the functionality found in SPARQL for reasoning over the web in a
federated manner. This limits the scope of data availability, integration and
ease of use for real world application. 

\subsubsection{RDF and OWL Integration}

While there is a lot of data stored in machine-processable formats on the semantic web, 
there is a lot of information existing in different data models. This 
tends to happen either because a particular technology was chosen due to 
a particular featureset, for historical reasons or simply by organisational convention.

Therefore, data integration must be performed on the datasets to use them
together and glean useful information from them. A major technology used for 
data integration on the semantic web is SPARQL.

As explained in the previous section - SPARQL is a flexible technology and allows 
data to be pulled from several different types of datastore (including Redis or an 
OWL ontology) through providing an IRI to resource; then converting it to an RDF 
expressed datastore, and allowing regular SPARQL querying in this way. 

This makes SPARQL a powerful tool for data integration, providing a common
language usable for querying over multiple types of datasets in a single query.

However, in the case of OWL ontologies, the current process of converting the
ontology to an RDF dataset carries a number of issues and reduced
functionalities when compared to working with OWL natively:

\begin{itemize}
  \item Inability to perform on-the-fly semantic reasoning on ontologies.
  \item No ability to query the data in a native manner (using Manchester OWL Syntax).
  \item Loss of certain descriptive logic modelling axioms available in OWL
  ontologies when expressed in RDF.
  \item Many OWL serialisation formats not supported by many RDF readers.
\end{itemize}

Combined, these factors reduce the quality and ease of querying OWL ontologies when 
using SPARQL. Therefore the problem exists in SPARQL that while it can pull data
from different sources, the process of doing so compromises some of the endemic
qualities of the integrated dataset - particularly in the case of OWL
ontologies.

\subsubsection{Importance and Impact}

Immediately, the previously mentioned drug discovery research using semantic web 
technologies largely involve integrating data which exists in different schemas or
different database systems to make use of the data and establish relationships\cite{semwebdiscovery}.

One thing that seems lacking in implementations of semantic web technologies as
they exist is that the major implementations are all rather large and opaque
Java libraries - this being a language which generally requires a lot of
boilerplate - additionally, transmission formats are commonly XML, which also
requires extra overhead in processing when compared to abstract key-value
storage formats such as JSON. 

Conversely, underlying technologies and applications on the web have meanwhile 
been moving towards use of lightweight frameworks and languages. Modern applications 
have been opting for abstract key-value stores such as Redis, which suffers from
a lack of additional features available from ontologies. 

Transmission of data on the web tends to favour simple REST APIs which feature 
simple HTTP requests with parameters, instead of the complicated XML structures 
necessary to query data from OWLLink servers for example. On the other hand, the
previously discussed SPARQL data endpoints provide a simple HTTP interface for
SPARQL queries, so while the underlying technology can be anything - the
interface means that any application can easily make use of the service.

The effectiveness of the availability of these simpler and more accessible 
technologies can be seen in the massive number of web and desktop applications
which make use of these standard interfaces and APIs.

Therefore, I believe it may be that the lack of takeup of knowledge-representation systems
for front-end applications may be the down to the lack of lightweight libraries, 
services and frameworks available for working with them. The solution of this
problem - particularly for OWL, which provides advanced knowledge representation
features and techniques - may potentially bring about some more interesting and
practical uses for the technology; hopefully moving towards the original vision
of the Semantic Web as a universally federated and easily traversable datastore
for all purposes.

* semantic web lacking simple software

\section{Design and Implementation}

My solution to the given problems are to develop an extension to the SPARQL
query language which allows a user to send a Manchester OWL Syntax query to a
remote OWL reasoner endpoint, which will return relevant objects which are
included in the SPARQL resultset. This will require both development of the
language extension and the queriable OWL endpoint.

\subsection{SPARQL Extension}

\subsubsection{Design}

As described in the State of The Art, there is already a SERVICE keyword
included in the SPARQL language which provides a good model for sending a text
query to a remote endpoint. Currently, it only supports remote SPARQL endpoints,
and it was decided not to extend this construct to include support for an OWL
endpoint as this would mean any resulting implementation would break the terms
of the SPARQL specification. However, it may be suggested to the W3C in future
to combine these constructs as a matter of efficiency. 

Therefore, my software will add an 'OWL' block to the SPARQL language, which
uses a similar syntactic pattern to the SERVICE keyword.

\begin{lstlisting}
OWL SILENT <http://realispicio.us:9090> {
    Pizza and hasTopping some FishTopping
}
\end{lstlisting}

There are three properties to the above query:

\begin{description}
    \item[OWL] The OWL keyword will designate the beginning of an OWL block
    denoting the beginning of the description for a query to be sent to a remote
    OWL endpoint.
    \item[SILENT] This is an optional keyword, which if included will mean that
    no results are integrated if a query fails but the query will continue,
    while if it isn't set and a query fails the SPARQL query will be halted and
    return an error message.
    \item[IRI] The IRI, contained between angled brackets following the OWL
    keyword, will define the URI of the remote OWL endpoint.
    \item[Query] Following, between the curly brackets, will be a Manchester OWL
    Syntax query referring to relevant objects to be returned to the query.
\end{description}

Upon parsing the OWL block the syntax will send a query to an OWL reasoner
endpoint (described next in this section), and integrate the returned results
into the query. 

\subsubsection{Implementation}

In researching the implementation of the OWL block addition to SPARQL, the
initial thoughts were to perform this by actually extending the language itself.
To do this, the only seemingly viable option was to extend Apache Jena, which is
a major and widely used semantic web framework written in Java which includes the 
ARQL SPARQL compiler. 

The original plan was to build the extra syntax into ARQL in Jena, and
distribute a modified version of the software while requesting the changes be
merged with the main codebase.

However, after further investigation this approach was deemed unsuitable because
of the perceived inextensibility of the ARQL codebase, which makes use of a
large amount of magic numbers and goes largely undocumented. [am I allowed to
say this?]

Therefore, a simpler approach was taken - to extend a library which sends a
SPARQL query to a SPARQL endpoint by preprocessing it to find and run OWL
blocks, then replacing the block with the results inline. The workflow of the
script will be the following:

\begin{enumerate}
    \item Input the SPARQL query into the script.
    \item Scan the script for OWL blocks (syntax described above).
    \item Run any OWL queries.
    \item Replace the OWL block with the resultset from the OWL query.
    \item Run the SPARQL query.
    \item Return the full resultset.
\end{enumerate}

In terms of inputting the SPARQL query into the processing script, it will
support being run both from the command line and over the web, by checking how
it's being run. From there, it will gain the input run from the command line
either designated in plaintext in the arguments, or from a file named in the
arguments; if run over the web, it will accept a POST parameter including the
query sent from a simple web interface also implemented by me (described later).

Scanning the script for occurences of OWL blocks will be done with a simple
Regex to build an array of all matches and involved parameters.

It will use these parameters to run each OWL block against the given OWL 
endpoint.

These results will then replace the OWL block in-place in the SPARQL query,
using the VALUES inline data syntax.

Once these have all been performed, the SPARQL query will be run against a
SPARQL endpoint and the result returned to the user using the appropriate method.

As an example, a simple SPARQL query including an OWL block like so (which
simply asks for classes related to the adaxial cell in a zebrafish ontology):

\begin{lstlisting}
SELECT * WHERE {}
OWL <http://realispicio.us:9090> {
    'adaxial cell'
}
\end{lstlisting}

Would first be processed to a SPARQL query string containing the OWL block
replaced by a VALUES block containing the results of the query:

\begin{lstlisting}
SELECT * WHERE {} 
VALUES (?iri ?id) { 
    ( http://purl.obolibrary.org/obo/#ZFA_0009000  ZFA_0009000 ) 
    ( http://www.w3.org/2002/07/owl#Nothing  Nothing )
}
\end{lstlisting}

\subsection{OWL Endpoint}

\subsubsection{Design}

The OWL endpoint will be a simple HTTP REST-like API which wraps code around an
OWL reasoner. The software will load a set of ontologies which any queries given
to the server will be run against, run the reasoner against them, and then set
up a simple server to accept Manchester OWL Syntax requests and return the
results in a machine-readable format.

\subsubsection{Implementation}

The initial stages of the program startup will be the following:

\begin{enumerate}
    \item Load given ontologies into an ontology manager. 
    \item Perform reasoning on a set of ontologies and store the results.
    \item Generate a list of short-form labels for the classes in the
    ontologies.
    \item Start a HTTP API listener with code to handle any queries.
\end{enumerate}

The workflow of a request made to the server will be as follows:

\begin{enumerate}
    \item Retrieve Manchester OWL Syntax query.
    \item Convert the Manchester OWL Syntax query to a class expression (able to
    be queried against an ontology).
    \item Retrieve relevant objects from the ontologies.
    \item Serialise these objects in a machine-readable format and return them 
    to the collector.
\end{enumerate}

OWLAPI will be used to handle ontologies, including the reasoning, Manchester
OWL Syntax conversion and class querying. 

For the server software I decided to use Jetty, since my research showed that
this was about the simplest HTTP server library available in Java which
implements the required functionality. It is well documented, and uses many
in-built Java API classes which works towards keeping the codebase light and
easily extensible by others. 

* not extensible
* should be on the side of the server

* Insert class diagram
* Workflow

\section{Testing}

\section{Conclusion}

\subsection{Summary of Contributions}

Implemented a demonstration of an extension to the SPARQL language to allow
remote querying of a set of OWL ontologies at
a given data endpoint using Manchester OWL Syntax. Relevant results are then
included in the OWL resultset and can
interact with the rest of the query as a regular 'list' of data. The script can
either be used from the command line or
through the web.

Implemented a basic Web front-end for making SPARQL queries, designed to make
available the previously noted implementation of
the OWL querying functionality on the Web.

Created a simple REST Web API to constitute an OWL data endpoint for ontology
querying, with a similar pattern to SPARQL
endpoints. It allows a user to make a Manchester OWL Syntax query, describing a
set of relevant classes, which are then
matched against a number of in-built and reasoned ontologies, then returned in
JSON format.

\subsection{Future Work}

\subsubsection{Re-implement current SPARQL functionality in other languages}

To increase the universal availability of this functionality, it would be a good
idea to implement it in other languages which
have a SPARQL library. The design pattern for the base OWL block functionality
is rather simple so it would be easy to port
to other languages either by myself or someone who needs it for a specific
purpose with a specific set of technologies. PHP
was chosen initially since the focus was on Web availability, and most web-based
SPARQL endpoints which humans interact with
are using PHP.

\subsubsection{Extend actual SPARQL Syntax}

Eventually, it would be a superior solution to actually extend the SPARQL
language itself - both by extension of the SPARQL
specification and through an extension in the reference implementation of SPARQL
(ARQ); this would increase visibility of the
project, as well as working towards the new features being more widely
supported.

\subsubsection{Combination with SERVICE}

As previously noted, the pattern between the SERVICE and OWL keywords are very
similar- namely, they both send a HTTP request and
use the results in the query. Because of this, it may be worth attempting to
extend the implementation of the SERVICE keyword such
that it supports OWL endpoints as well as SPARQL endpoints. The advantages of
this would be syntactic efficiency, as well as providing
a reasonable and extendable framework for supporting other such simple endpoints
in future (integration from other types of data).

\subsubsection{Data Integration From Other Sources}

While connecting OWL and RDF datastores in this manner already allows for a lot
of new
works to be done, there are other semantic data
formats which currently do not support easy data integration. I think that
SPARQL serves
as a good base for any further data integration,
given its wide adoption and currently existing support for many different types
of data.

One particular use of this could be to undertake a project to use this method of
data
integration among others to explore the the phenotypic similarites between "loss
of
function" mutations from gene knockout animal models and the inhibition of
protein
functions by existing drugs.

Through doing this, one may potentially discover novel drug actions and
indications -
with the furhter potential to eventually find new treatments for currently
untreatable
conditions; an approach which may be seen as particularly favourable in recent
times
due to the heightening cost of research and development of entirely new drugs.

It would be achieved by first integrating drug effect and model organism
phenotype data
from public databases through various currently existing and newly developed
data
integration techniques, then determine any similarities between drug effects and
organism phenotypes. Once these are identified, one would determine whether the
observed similarities can provide insights into drug-target relationships.

TODO examples

* More practical uses for the implemented functionality

\subsubsection{More Functionality in the OWL API}

The current implementation of the OWL API endpoint is very simple, designed as
what is mainly a proof of concept. As mentioned in the design
section, the API currently searches objects over a set of default ontologies
found at the\cite{obofoundry}. While this provides a useful set of
ontologies for queries, it is somewhat inflexible. Currently, if a user wishes
to query an ontology which isn't included in the endpoint, they
would have to download a copy of the software and modify the list of ontologies
- then running it themselves.

To remedy this, I would plan on keeping the current functionality in terms of a
default set of ontologies, but implement an additional query
parameter which allows the user to list a set of URIs pointing at ontologies to
reason and query. While this would be a useful feature, it
presents several design challenges - such as security (care would have to be
taken so that a user could not break the server by pointing it
at some malicious form of broken ontology, or crash the software by passing an
ontology too large for the computer to handle), and additionally
it would be difficult to succinctly include a list of ontologies in an OWL block
- for these purposes it may be better to run different endpoints
for different sets of relevant and interrelated ontologies.

Whilst going forward with this project, it may also be worth drafting a formal
specification for what constitutes an 'OWL endpoint,' similar
to that which exists for SPARQL endpoints\cite. This would be useful for an
authoritative source on functionality, as well as being able
to dictate necessary functionality for any further and alternative
implementations.

\subsubsection{Possibly Extend OWLLink API}

Despite not supporting Manchester OWL Syntax queries, OWLLink API is a large and
developed project. For this reason, it may be worth doing some
more work into researching whether the implemtation of Manchester OWL Syntax
queries is faesible.

\begin{thebibliography}{9}

\bibitem{semweb}
  Tim Berners-Lee et al,
  \emph{The Semantic Web}.
  Feature Article, Scientific American
  May 2001.

\bibitem{rdfprimer}
 Tim Berners-Lee et al,
 \emph{RDF Primer}.
 W3C,
 2004.

\bibitem{sparql}
  Eric Prud'hommeaux, Andy Seaborne,
  \emph{SPARQL Query Language for RDF}.
  W3C,
  January 2008.

\bibitem{desclogic}
 Franz Baader, Deborah L. McGuinness, Daniele Nardi, Peter F. Patel-Schneider,
 \emph{The Description Logic Handbook - Theory, Implementation and Actions}.
 Cambridge University Press,
 March 2003.

\bibitem{owlprimer}
  Pascal Hitzler et al,
  \emph{OWL 2 Web Ontology Language Primer (Second Edition)}.
  W3C,
  December 2012.

\bibitem{humontology}
  Nicole L. Washington et al,
  Linking Human Diseases to Animal Models Using Ontology-Based Phenotype Annotation,
  Plos Biology,
  November 2009.

\bibitem{semwebdiscovery}
  D.J. Wild, et al.,
  Systems chemical biology and the Semantic Web: what they mean for the future of drug discovery research,
  Drug Discov Today,
  2012.

http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1000247#pbio-1000247-g007

\bibitem{obofoundry}
  Obo Foundry
  http://obofoundry.org/
  Retrieved 17/04/2014.

\bibitem{dbpedia-uses}
  DBPedia Use Cases
  http://wiki.dbpedia.org/UseCases
  Retrieved 07/04/2014

Example OWLLink query:
http://owllink.org/owllink-httpxml-20091116/additional-examples/owllink-example-ClassQueries-request-20091116.xml

\end{thebibliography}

\end{document}
